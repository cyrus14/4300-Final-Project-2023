{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import wiki_scraping\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('song_tf_idf.npy', 'rb') as f:\n",
    "    song_tf_idf = np.load(f)\n",
    "with open('word_to_index.pkl', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "with open('song_to_index.pkl', 'rb') as f:\n",
    "    song_to_index = pickle.load(f)\n",
    "with open('index_to_song.pkl', 'rb') as f:\n",
    "    index_to_song = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset/wiki_scraping/wiki_texts.json')\n",
    "wiki_texts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tf_idf = np.zeros((len(wiki_texts), song_tf_idf.shape[1]))\n",
    "for i, city in enumerate(wiki_texts.keys()):\n",
    "    txt = wiki_texts[city]\n",
    "    for wrd in txt:\n",
    "        if wrd in word_to_index:\n",
    "            j = word_to_index[wrd]\n",
    "            wiki_tf_idf[i, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tf_idf = (wiki_tf_idf + 0.5) / (wiki_tf_idf.sum(axis=0) + 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_to_index = {cty:i for i, cty in enumerate(wiki_texts.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_tf_idf.npy', 'wb') as f:\n",
    "    np.save(f, wiki_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(city, song):\n",
    "    city_i = loc_to_index[city]\n",
    "    song_i = song_to_index[song]\n",
    "    city_vec = wiki_tf_idf[city_i, :]\n",
    "    song_vec = song_tf_idf[song_i, :]\n",
    "    denom = np.linalg.norm(city_vec) * np.linalg.norm(song_vec)\n",
    "    num = city_vec @ song_vec\n",
    "    return (num + 0.5) /  (denom + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_matches = []\n",
    "for loc in loc_to_index:\n",
    "    sim = cos_sim(loc, \"A Milli\")\n",
    "    best_matches.append((loc, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto\n",
      "Song:  New York City   Score: 0.813875\n",
      "Song:  London   Score: 0.811230\n",
      "Song:  Budapest   Score: 0.811032\n",
      "Song:  Toronto   Score: 0.810856\n",
      "Song:  Mumbai   Score: 0.810727\n",
      "Song:  Tokyo   Score: 0.808863\n"
     ]
    }
   ],
   "source": [
    "print(\"Toronto\")\n",
    "srtd = sorted(best_matches, key = lambda x: x[1], reverse=True)\n",
    "for t in srtd[:10]:\n",
    "    print(\"Song: \", t[0], \"  Score: {:.6f}\".format(t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
